{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:52:08.813701Z",
     "start_time": "2021-11-16T17:52:07.173211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Get teh data\n",
    "iris  = load_iris()\n",
    "features = iris.feature_names\n",
    "df = pd.DataFrame(iris.data, columns=features)\n",
    "df[\"target\"] = iris.target\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:39.365885Z",
     "start_time": "2021-11-16T17:53:39.175680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict for {\n",
      "    \"sepal length (cm)\": 5.1,\n",
      "    \"sepal width (cm)\": 3.5,\n",
      "    \"petal length (cm)\": 1.4,\n",
      "    \"petal width (cm)\": 0.2\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "\n",
       "   prediction  \n",
       "0           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from goldilox import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "pipeline = Pipeline.from_sklearn(CatBoostClassifier(verbose=0)).fit(df[features], df[\"target\"])\n",
    "\n",
    "# I/O Example\n",
    "raw = pipeline.raw\n",
    "print(f\"predict for {json.dumps(raw, indent=4)}\")\n",
    "pipeline.inference(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation    \n",
    "We can see the pipeline is valid, but cannot handle missing value if they happen in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:40.237830Z",
     "start_time": "2021-11-16T17:53:40.203322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variabels and description\n",
    "We can add variables which want to assosiate with the pipeline, and a description.\n",
    "* A greate place to put the training params, evaluation results, version, branch, etc,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:42.030330Z",
     "start_time": "2021-11-16T17:53:42.027724Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.description = \"LightGBM on the iris dataset with sklearn\"\n",
    "pipeline.variables[\"var1\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:59.076175Z",
     "start_time": "2021-11-16T17:53:47.192783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../tests/models/server.pkl\n",
      "Check out the docs: http://127.0.0.1:5000\n",
      "\n",
      "[2021-11-16 18:53:50 +0100] [74866] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-11-16 18:53:50 +0100] [74866] [INFO] Listening at: http://127.0.0.1:5000 (74866)\n",
      "[2021-11-16 18:53:50 +0100] [74866] [INFO] Using worker: uvicorn.workers.UvicornH11Worker\n",
      "[2021-11-16 18:53:50 +0100] [74872] [INFO] Booting worker with pid: 74872\n",
      "[2021-11-16 18:53:50 +0100] [74872] [INFO] Started server process [74872]\n",
      "[2021-11-16 18:53:50 +0100] [74872] [INFO] Waiting for application startup.\n",
      "[2021-11-16 18:53:50 +0100] [74872] [INFO] Application startup complete.\n",
      "^C\n",
      "[2021-11-16 18:53:58 +0100] [74866] [INFO] Handling signal: int\n",
      "[2021-11-16 18:53:58 +0100] [74866] [WARNING] Worker with pid 74872 was terminated due to signal 3\n",
      "[2021-11-16 18:53:58 +0100] [74866] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved to: {pipeline.save(\"../tests/models/server.pkl\")}\")\n",
    "print(f\"Check out the docs: http://127.0.0.1:5000\\n\")\n",
    "!gl serve ../tests/models/server.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaex solution\n",
    "\n",
    "Vaex solutions are much more **powerful** and allow for easier feature engineering and scale.    \n",
    "In this example we do a simple feature engineering, and process the results to labels, so it would be easier to consume on the frontend side.\n",
    "\n",
    "* We do not need to implement transformers for each feature engineering step or estimators. Instead we create simple functions which does what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:54:39.349564Z",
     "start_time": "2021-11-16T17:54:33.655135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline raw data example:\n",
      "{\n",
      "    \"sepal_length\": 5.9,\n",
      "    \"sepal_width\": 3.0,\n",
      "    \"petal_length\": 4.2,\n",
      "    \"petal_width\": 1.5\n",
      "}\n",
      "\n",
      "Pipeline output example:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  sepal_length</th><th style=\"text-align: right;\">  sepal_width</th><th style=\"text-align: right;\">  petal_length</th><th style=\"text-align: right;\">  petal_width</th><th style=\"text-align: right;\">  class_</th><th style=\"text-align: right;\">  petal_ratio</th><th>predictions                                        </th><th style=\"text-align: right;\">  prediction</th><th>label     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">           5.9</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">           4.2</td><td style=\"text-align: right;\">          1.5</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">      2.8    </td><td>&#x27;array([6.09742414e-07, 9.99998377e-01, 1.013054...</td><td style=\"text-align: right;\">           1</td><td>versicolor</td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>1</i></td><td style=\"text-align: right;\">           6.1</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">           4.6</td><td style=\"text-align: right;\">          1.4</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">      3.28571</td><td>&#x27;array([5.37810099e-07, 9.99998682e-01, 7.801238...</td><td style=\"text-align: right;\">           1</td><td>versicolor</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    sepal_length    sepal_width    petal_length    petal_width    class_    petal_ratio  predictions                                            prediction  label\n",
       "  0             5.9              3             4.2            1.5         1        2.8      'array([6.09742414e-07, 9.99998377e-01, 1.013054...             1  versicolor\n",
       "  1             6.1              3             4.6            1.4         1        3.28571  'array([5.37810099e-07, 9.99998682e-01, 7.801238...             1  versicolor"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vaex\n",
    "import warnings\n",
    "from vaex.ml.datasets import load_iris_1e5\n",
    "from vaex.ml.catboost import CatBoostModel\n",
    "from goldilox import Pipeline\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df = load_iris_1e5()\n",
    "target = \"class_\"\n",
    "\n",
    "# feature engineering example\n",
    "df[\"petal_ratio\"] = df[\"petal_length\"] / df[\"petal_width\"] \n",
    "\n",
    "booster = CatBoostModel(features=[\"petal_length\", \"petal_width\", \"sepal_length\", \"sepal_width\", \"petal_ratio\"],\n",
    "                        target=target,\n",
    "                        prediction_name=\"predictions\",\n",
    "                        params={\"num_boost_round\":500, \"verbose\":0, \"objective\":\"MultiClass\"})\n",
    "                        \n",
    "booster.fit(df)\n",
    "df = booster.transform(df)\n",
    "\n",
    "# post model processing example\n",
    "@vaex.register_function()\n",
    "def argmax(ar, axis=1):\n",
    "    return np.argmax(ar, axis=axis)\n",
    "df.add_function(\"argmax\", argmax)\n",
    "df[\"prediction\"] = df[\"predictions\"].argmax()\n",
    "\n",
    "df[\"label\"] = df[\"prediction\"].map({0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"})\n",
    "\n",
    "# # Vaex remember all the transformations, this is a skleran.pipeline alternative\n",
    "pipeline = Pipeline.from_vaex(df, description=\"simple Catboost\")\n",
    "pipeline.raw.pop(target) # (optional) we don\"t expect to get the class_ in queries\n",
    "assert pipeline.validate()\n",
    "print(\"Pipeline raw data example:\")\n",
    "print(json.dumps(pipeline.raw, indent=4))\n",
    "print(\"\")\n",
    "print(\"Pipeline output example:\")\n",
    "pipeline.inference(pipeline.raw).to_records()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:54:55.395720Z",
     "start_time": "2021-11-16T17:54:41.417069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../tests/models/server.pkl\n",
      "Check out the docs: http://127.0.0.1:5000\n",
      "\n",
      "[2021-11-16 18:54:44 +0100] [74906] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-11-16 18:54:44 +0100] [74906] [INFO] Listening at: http://127.0.0.1:5000 (74906)\n",
      "[2021-11-16 18:54:44 +0100] [74906] [INFO] Using worker: uvicorn.workers.UvicornH11Worker\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Booting worker with pid: 74911\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Started server process [74911]\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Waiting for application startup.\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Application startup complete.\n",
      "^C\n",
      "[2021-11-16 18:54:54 +0100] [74906] [INFO] Handling signal: int\n",
      "[2021-11-16 18:54:54 +0100] [74906] [WARNING] Worker with pid 74911 was terminated due to signal 3\n",
      "[2021-11-16 18:54:55 +0100] [74906] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved to: {pipeline.save(\"../tests/models/server.pkl\")}\")\n",
    "print(f\"Check out the docs: http://127.0.0.1:5000\\n\")\n",
    "\n",
    "!gl serve ../tests/models/server.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance   \n",
    "Let\"s have a look at an edance training function, which we want to re-run when new data arrives.     \n",
    "To implement this, we must everything within a function which recive a dataframe and return a Vaex DataFrame\n",
    "\n",
    "The function:    \n",
    "First we run a \"*random_split*\" experiment and save the results.    \n",
    "Next, we train the data on the entire dataset.    \n",
    "Finally, we add the evalution as a varaible so we can recall how good the model was.\n",
    "\n",
    "\n",
    "* This way we can change the pipeline training and outputs without changing our infrastructure at all.\n",
    "* This also create a model for production who learned from the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:00:42.722945Z",
     "start_time": "2021-11-16T18:00:41.975205Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from vaex.ml.datasets import load_iris\n",
    "from goldilox import Pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # lightgbm fun\n",
    "\n",
    "def fit(df):\n",
    "    import vaex\n",
    "    import numpy as np\n",
    "    from vaex.ml.catboost import CatBoostModel\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from goldilox import Pipeline\n",
    "\n",
    "    train, test = df.ml.train_test_split(test_size=0.2, verbose=False)        \n",
    "    target = \"class_\"\n",
    "    prediction_name = \"predictions\"\n",
    "    \n",
    "    train[\"petal_ratio\"] = train[\"petal_length\"] / train[\"petal_width\"] \n",
    "    \n",
    "    features = [\"petal_length\", \"petal_width\", \"sepal_length\", \"sepal_width\", \"petal_ratio\"]    \n",
    "    params = {\"num_boost_round\":500, \"verbose\":0, \"objective\":\"MultiClass\"}\n",
    "    booster = CatBoostModel(features=features,\n",
    "                        target=target,\n",
    "                        prediction_name=prediction_name,\n",
    "                        params=params)    \n",
    "    booster.fit(train)    \n",
    "\n",
    "    @vaex.register_function()\n",
    "    def argmax(ar, axis=1):\n",
    "        return np.argmax(ar, axis=axis)\n",
    "\n",
    "    train = booster.transform(train)\n",
    "    train.add_function(\"argmax\", argmax)\n",
    "    train[\"prediction\"] = train[\"predictions\"].argmax()\n",
    "    \n",
    "    \"\"\"\n",
    "    Using the  way to get predictions on a new dataset.\n",
    "    This is very helpful if we did many feature engineering transformations. \n",
    "    \"\"\"\n",
    "    pipeline = Pipeline.from_vaex(train) \n",
    "    accuracy = accuracy_score(pipeline.inference(test)[\"prediction\"].values,\n",
    "                              test[target].values)\n",
    "    \n",
    "    # Re-train on the entire dataset\n",
    "    booster = CatBoostModel(features=features,\n",
    "                        target=target,\n",
    "                        prediction_name=prediction_name,\n",
    "                        params=params)\n",
    "    processed = pipeline.inference(df) # all feature engineering (including the model which we will overite)\n",
    "    booster.fit(processed)\n",
    "    df = booster.transform(processed)\n",
    "    df.add_function(\"argmax\", argmax)\n",
    "    df[\"prediction\"] = df[prediction_name].argmax()\n",
    "    # The \"label\" is to help the Frontend app to understand what actually was the result\n",
    "    df[\"label\"] = df[\"prediction\"].map({0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"})\n",
    "    df.variables[\"accuracy\"] = accuracy\n",
    "    return df\n",
    "\n",
    "df = load_iris()\n",
    "pipeline = Pipeline.from_vaex(df, fit=fit).fit(df)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:00:43.880434Z",
     "start_time": "2021-11-16T18:00:43.827868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  sepal_length</th><th style=\"text-align: right;\">  sepal_width</th><th style=\"text-align: right;\">  petal_length</th><th style=\"text-align: right;\">  petal_width</th><th style=\"text-align: right;\">  class_</th><th style=\"text-align: right;\">  petal_ratio</th><th>predictions                                </th><th style=\"text-align: right;\">  prediction</th><th>label     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">           5.9</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">           4.2</td><td style=\"text-align: right;\">          1.5</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">          2.8</td><td>array([0.00121612, 0.99772101, 0.00106287])</td><td style=\"text-align: right;\">           1</td><td>versicolor</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    sepal_length    sepal_width    petal_length    petal_width    class_    petal_ratio  predictions                                    prediction  label\n",
       "  0             5.9              3             4.2            1.5         1            2.8  array([0.00121612, 0.99772101, 0.00106287])             1  versicolor"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "path = str(TemporaryDirectory().name) + \"/model.pkl\"\n",
    "pipeline.save(path)\n",
    "pipeline = Pipeline.from_file(path)\n",
    "\n",
    "pipeline.inference(pipeline.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:00:51.709816Z",
     "start_time": "2021-11-16T18:00:46.584400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0 - very good as we duplicated the original data (:\n"
     ]
    }
   ],
   "source": [
    "from vaex.ml.datasets import load_iris_1e5\n",
    "\n",
    "df = load_iris_1e5() # iris 670 times\n",
    "pipeline.fit(df)\n",
    "assert pipeline.validate()\n",
    "print(f\"Accuracy: {pipeline.variables['accuracy']} - very good as we duplicated the original data (:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve\n",
    "\n",
    "* Note that when we train in this way, the \"*raw*\" example has the target variable \"class_\" which we will not expect in production.  This is no issue, we can either \"pop\" it out from the pipeline.raw, or just ignore it, predictions still work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:01:46.165013Z",
     "start_time": "2021-11-16T18:01:22.732040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../tests/models/server.pkl\n",
      "Check out the docs: http://127.0.0.1:5000\n",
      "\n",
      "[2021-11-16 19:01:25 +0100] [75207] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-11-16 19:01:25 +0100] [75207] [INFO] Listening at: http://127.0.0.1:5000 (75207)\n",
      "[2021-11-16 19:01:25 +0100] [75207] [INFO] Using worker: uvicorn.workers.UvicornH11Worker\n",
      "[2021-11-16 19:01:25 +0100] [75213] [INFO] Booting worker with pid: 75213\n",
      "[2021-11-16 19:01:26 +0100] [75213] [INFO] Started server process [75213]\n",
      "[2021-11-16 19:01:26 +0100] [75213] [INFO] Waiting for application startup.\n",
      "[2021-11-16 19:01:26 +0100] [75213] [INFO] Application startup complete.\n",
      "^C\n",
      "[2021-11-16 19:01:45 +0100] [75207] [INFO] Handling signal: int\n",
      "[2021-11-16 19:01:45 +0100] [75207] [WARNING] Worker with pid 75213 was terminated due to signal 3\n",
      "[2021-11-16 19:01:45 +0100] [75207] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "pipeline.raw.pop(\"class_\", None) # we can also leave it unpoped\n",
    "print(f\"Saved to: {pipeline.save(\"../tests/models/server.pkl\")}\")\n",
    "print(f\"Check out the docs: http://127.0.0.1:5000\\n\")\n",
    "\n",
    "!gl serve ../tests/models/server.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
