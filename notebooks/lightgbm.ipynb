{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LightGBM](https://lightgbm.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:29:52.458666Z",
     "start_time": "2022-01-13T11:29:51.142177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0           5.1          3.5           1.4          0.2       0\n",
       "1           4.9          3.0           1.4          0.2       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "from goldilox.datasets import load_iris\n",
    "\n",
    "# Get teh data\n",
    "df, features, target = load_iris()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:29:55.050658Z",
     "start_time": "2022-01-13T11:29:52.461578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Pipeline doesn't handle NA for sepal_length\n",
      "WARNING: Pipeline doesn't handle NA for sepal_width\n",
      "WARNING: Pipeline doesn't handle NA for petal_length\n",
      "WARNING: Pipeline doesn't handle NA for petal_width\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  prediction\n",
       "0           5.1          3.5           1.4          0.2           0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from goldilox import Pipeline\n",
    "\n",
    "X, y = df[features], df[target]\n",
    "model = LGBMClassifier()\n",
    "\n",
    "pipeline = Pipeline.from_sklearn(model).fit(X, y)\n",
    "\n",
    "# I/O Example\n",
    "pipeline.inference(pipeline.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are not handled, Let's fix this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:30:00.450226Z",
     "start_time": "2022-01-13T11:29:55.053424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sepal_length  sepal_width  petal_length  petal_width  prediction\n",
       "0         None          3.5           1.4          0.2           0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from goldilox.sklearn.tranformers import Imputer\n",
    "import sklearn.pipeline\n",
    "\n",
    "skleran_pipeline = sklearn.pipeline.Pipeline([('imputer', Imputer()),\n",
    "                                              ('classifier', LGBMClassifier())])\n",
    "pipeline = Pipeline.from_sklearn(skleran_pipeline).fit(X, y)\n",
    "\n",
    "# I/O Example\n",
    "pipeline.inference({'sepal_length': None,\n",
    "                    'sepal_width': 3.5,\n",
    "                    'petal_length': 1.4,\n",
    "                    'petal_width': 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and description\n",
    "We can add variables which we want to associate with the pipeline, and a description.\n",
    "* A great place to put the training params, evaluation results, version, branch, etc,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:30:00.460032Z",
     "start_time": "2022-01-13T11:30:00.451674Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline.set_variable('training_accuracy', accuracy_score(y, pipeline.inference(X)['prediction']))\n",
    "pipeline.description = \"LightGBM on the iris dataset with sklearn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this example I run the evaluation on the same data as training, ofc you would want to split the data to train/test, or kfold etc,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaex solution\n",
    "\n",
    "Vaex solutions are much more **powerful** and allow for easier feature engineering and scale.    \n",
    "In this example we do a simple feature engineering, and process the results to labels, so it would be easier to consume on the frontend side.\n",
    "\n",
    "* We do not need to implement transformers for each feature engineering step or estimators. Instead we create simple functions which does what we want.\n",
    "* It's good to remember that whenever we do train a model which loads **all data to memory**, Vaex obviously doesn't prevant that, and it should be taken into account - Maybe [online learning](https://docs.goldilox.io/reference/data-science-examples/online-learning)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:37:29.927819Z",
     "start_time": "2022-01-13T11:37:27.258629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  sepal_length</th><th style=\"text-align: right;\">  sepal_width</th><th style=\"text-align: right;\">  petal_length</th><th style=\"text-align: right;\">  petal_width</th><th style=\"text-align: right;\">  target</th><th style=\"text-align: right;\">  petal_ratio</th><th>predictions                                        </th><th style=\"text-align: right;\">  prediction</th><th>label  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">           5.1</td><td style=\"text-align: right;\">          3.5</td><td style=\"text-align: right;\">           1.4</td><td style=\"text-align: right;\">          0.2</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">            7</td><td>&#x27;array([9.99999943e-01, 5.70021072e-08, 4.053823...</td><td style=\"text-align: right;\">           0</td><td>setosa </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    sepal_length    sepal_width    petal_length    petal_width    target    petal_ratio  predictions                                            prediction  label\n",
       "  0             5.1            3.5             1.4            0.2         0              7  'array([9.99999943e-01, 5.70021072e-08, 4.053823...             0  setosa"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vaex\n",
    "import warnings\n",
    "from vaex.ml.lightgbm import LightGBMModel\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = vaex.from_pandas(df)\n",
    "\n",
    "# feature engineering example\n",
    "df['petal_ratio'] = df['petal_length'] / df['petal_width']\n",
    "\n",
    "features = features + ['petal_ratio']\n",
    "booster = LightGBMModel(features=features,\n",
    "                        target=target,\n",
    "                        prediction_name='predictions',\n",
    "                        num_boost_round=500, params={'verbosity': -1,\n",
    "                                                     'objective': 'multiclass',\n",
    "                                                     'num_class': 3})\n",
    "booster.fit(df)\n",
    "df = booster.transform(df)\n",
    "\n",
    "\n",
    "# post model processing example\n",
    "@vaex.register_function()\n",
    "def argmax(ar, axis=1):\n",
    "    return np.argmax(ar, axis=axis)\n",
    "\n",
    "\n",
    "df.add_function('argmax', argmax)\n",
    "df['prediction'] = df['predictions'].argmax()\n",
    "\n",
    "classes = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "df['label'] = df['prediction'].map(classes)\n",
    "\n",
    "# Vaex remember all the transformations, this is a skleran.pipeline alternative\n",
    "pipeline = Pipeline.from_vaex(df, description='simple lightGBM')\n",
    "\n",
    "pipeline.inference(pipeline.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaex + sklearn  -> vaex.ml.sklearn.Predictor\n",
    "Another way to wrap any sklearn model into Vaex with a *Predictor* wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:37:31.585787Z",
     "start_time": "2022-01-13T11:37:29.929457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  sepal_length</th><th style=\"text-align: right;\">  sepal_width</th><th style=\"text-align: right;\">  petal_length</th><th style=\"text-align: right;\">  petal_width</th><th>target  </th><th style=\"text-align: right;\">  petal_ratio</th><th>predictions                                        </th><th style=\"text-align: right;\">  prediction</th><th>label  </th><th style=\"text-align: right;\">  lgbm2</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">           5.1</td><td style=\"text-align: right;\">          3.5</td><td style=\"text-align: right;\">           1.4</td><td style=\"text-align: right;\">          0.2</td><td>--      </td><td style=\"text-align: right;\">            7</td><td>&#x27;array([9.99999943e-01, 5.70021072e-08, 4.053823...</td><td style=\"text-align: right;\">           0</td><td>setosa </td><td style=\"text-align: right;\">      0</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    sepal_length    sepal_width    petal_length    petal_width  target      petal_ratio  predictions                                            prediction  label      lgbm2\n",
       "  0             5.1            3.5             1.4            0.2  --                    7  'array([9.99999943e-01, 5.70021072e-08, 4.053823...             0  setosa         0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaex.ml.sklearn import Predictor\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "model = Predictor(model=LGBMClassifier(), features=features, target=target, prediction_name='lgbm2')\n",
    "model.fit(df)\n",
    "df = model.transform(df)\n",
    "pipeline = Pipeline.from_vaex(df, description='simple lightGBM')\n",
    "pipeline.raw.pop(target)\n",
    "\n",
    "pipeline.inference(pipeline.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need even more flexibility? -> @vaex.register_function()\n",
    "This is the Vaex swisse-knife \n",
    "* works with any pickable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:38:30.061244Z",
     "start_time": "2022-01-13T11:38:28.424402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  sepal_length</th><th style=\"text-align: right;\">  sepal_width</th><th style=\"text-align: right;\">  petal_length</th><th style=\"text-align: right;\">  petal_width</th><th style=\"text-align: right;\">  target</th><th style=\"text-align: right;\">  petal_ratio</th><th>predictions                                        </th><th style=\"text-align: right;\">  prediction</th><th>label  </th><th style=\"text-align: right;\">  lgbm2</th><th>probabilities                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">           5.1</td><td style=\"text-align: right;\">          3.5</td><td style=\"text-align: right;\">           1.4</td><td style=\"text-align: right;\">          0.2</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">            7</td><td>&#x27;array([9.99999943e-01, 5.70021072e-08, 4.053823...</td><td style=\"text-align: right;\">           0</td><td>setosa </td><td style=\"text-align: right;\">      0</td><td>&quot;{&#x27;setosa&#x27;: 0.9999970258541964, &#x27;versicolor&#x27;: 2....</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    sepal_length    sepal_width    petal_length    petal_width    target    petal_ratio  predictions                                            prediction  label      lgbm2  probabilities\n",
       "  0             5.1            3.5             1.4            0.2         0              7  'array([9.99999943e-01, 5.70021072e-08, 4.053823...             0  setosa         0  \"{'setosa': 0.9999970258541964, 'versicolor': 2...."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "model = LGBMClassifier().fit(df[features].values, df[target].values)\n",
    "\n",
    "\n",
    "@vaex.register_function()\n",
    "def predict_proba(*columns):\n",
    "    X = np.array(columns).T  # Vaex retrive the batches efficiently, but transposed\n",
    "    probabilities = model.predict_proba(X)\n",
    "    return pa.array([{classes.get(i): probability for i, probability in enumerate(row)} for row in probabilities])\n",
    "\n",
    "\n",
    "df.add_function('predict_proba', predict_proba)\n",
    "df['probabilities'] = df.func.predict_proba(*features)\n",
    "\n",
    "pipeline = Pipeline.from_vaex(df)\n",
    "pipeline.inference(pipeline.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Serve](https://docs.goldilox.io/reference/api-reference/cli/serve)\n",
    "All pipeline get a predictions server in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T11:22:24.260486Z",
     "start_time": "2022-01-13T11:21:38.175490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: pipeline.pkl\n",
      "Check out the docs: http://127.0.0.1:5000/docs\n",
      "\n",
      "[2022-01-13 13:21:43 +0200] [20124] [INFO] Starting gunicorn 20.1.0\n",
      "[2022-01-13 13:21:43 +0200] [20124] [INFO] Listening at: http://127.0.0.1:8000 (20124)\n",
      "[2022-01-13 13:21:43 +0200] [20124] [INFO] Using worker: uvicorn.workers.UvicornH11Worker\n",
      "[2022-01-13 13:21:43 +0200] [20137] [INFO] Booting worker with pid: 20137\n",
      "[2022-01-13 13:21:44 +0200] [20137] [INFO] Started server process [20137]\n",
      "[2022-01-13 13:21:44 +0200] [20137] [INFO] Waiting for application startup.\n",
      "[2022-01-13 13:21:44 +0200] [20137] [INFO] Application startup complete.\n",
      "^C\n",
      "[2022-01-13 13:22:23 +0200] [20124] [INFO] Handling signal: int\n",
      "[2022-01-13 13:22:23 +0200] [20137] [INFO] Shutting down\n",
      "[2022-01-13 13:22:23 +0200] [20137] [INFO] Waiting for application shutdown.\n",
      "[2022-01-13 13:22:23 +0200] [20137] [INFO] Application shutdown complete.\n",
      "[2022-01-13 13:22:23 +0200] [20137] [INFO] Finished server process [20137]\n",
      "[2022-01-13 13:22:23 +0200] [20137] [INFO] Worker exiting (pid: 20137)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved to: {pipeline.save('pipeline.pkl')}\")\n",
    "print(f\"Check out the docs: http://127.0.0.1:8000/docs\\n\")\n",
    "!glx serve pipeline.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}