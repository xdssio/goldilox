{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:52:08.813701Z",
     "start_time": "2021-11-16T17:52:07.173211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Get teh data\n",
    "iris = load_iris()\n",
    "features = iris.feature_names\n",
    "df = pd.DataFrame(iris.data, columns=features)\n",
    "df[\"target\"] = iris.target\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "To turn the model into a pipeline for production - Oneliner (-:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:39.365885Z",
     "start_time": "2021-11-16T17:53:39.175680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict for {\n",
      "    \"sepal length (cm)\": 5.1,\n",
      "    \"sepal width (cm)\": 3.5,\n",
      "    \"petal length (cm)\": 1.4,\n",
      "    \"petal width (cm)\": 0.2\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "\n",
       "   prediction  \n",
       "0           0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from goldilox import Pipeline\n",
    "\n",
    "pipeline = Pipeline.from_sklearn(XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")).fit(df[features], df[\"target\"])\n",
    "\n",
    "# I/O Example\n",
    "raw = pipeline.raw\n",
    "print(f\"predict for {json.dumps(raw, indent=4)}\")\n",
    "pipeline.inference(raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation    \n",
    "We can see the pipeline is valid, but cannot handle missing value if they happen in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:40.237830Z",
     "start_time": "2021-11-16T17:53:40.203322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline doesn't handle na for sepal length (cm)\n",
      "Pipeline doesn't handle na for sepal width (cm)\n",
      "Pipeline doesn't handle na for petal length (cm)\n",
      "Pipeline doesn't handle na for petal width (cm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix this!\n",
    "\n",
    "* Note that when we create a pipeline from transformers and estomators which are already **trained**, we need to add the \"*raw*\" parameters ourself.   \n",
    "This allow us to validate the data and create code examples for the docs later on.  \n",
    "* This is becuase the SImpleImputer has a bug and cannpt trained in a simple pipeline (A workaround is [here](https://stackoverflow.com/questions/51741873/sklearns-simpleimputer-doesnt-work-in-a-pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:41.071212Z",
     "start_time": "2021-11-16T17:53:41.025819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    " \n",
    "imputer = ColumnTransformer([('features_mean', SimpleImputer(strategy='mean'), features)], remainder='passthrough')\n",
    "skleran_pipeline = SklearnPipeline([(\"imputer\", imputer), (\"classifier\", model)]).fit(df[features], df[\"target\"])\n",
    "pipeline = Pipeline.from_sklearn(skleran_pipeline, raw=raw)\n",
    "pipeline.validate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variabels and description\n",
    "We can add variables which want to assosiate with the pipeline, and a description.\n",
    "* A greate place to put the training params, evaluation results, version, branch, etc,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:42.030330Z",
     "start_time": "2021-11-16T17:53:42.027724Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.description = \"LightGBM on the iris dataset with sklearn\"\n",
    "pipeline.variables[\"var1\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:53:59.076175Z",
     "start_time": "2021-11-16T17:53:47.192783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../tests/models/server.pkl\n",
      "Check out the docs: http://127.0.0.1:5000\n",
      "\n",
      "[2021-11-25 16:53:34 +0100] [3066] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-11-25 16:53:34 +0100] [3066] [INFO] Listening at: http://127.0.0.1:5000 (3066)\n",
      "[2021-11-25 16:53:34 +0100] [3066] [INFO] Using worker: uvicorn.workers.UvicornH11Worker\n",
      "[2021-11-25 16:53:34 +0100] [3090] [INFO] Booting worker with pid: 3090\n",
      "[2021-11-25 16:53:34 +0100] [3090] [INFO] Started server process [3090]\n",
      "[2021-11-25 16:53:34 +0100] [3090] [INFO] Waiting for application startup.\n",
      "[2021-11-25 16:53:34 +0100] [3090] [INFO] Application startup complete.\n",
      "^C\n",
      "[2021-11-25 16:54:53 +0100] [3066] [INFO] Handling signal: int\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved to: {pipeline.save('../tests/models/server.pkl')}\")\n",
    "print(f\"Check out the docs: http://127.0.0.1:5000/docs\\n\")\n",
    "!gl serve ../tests/models/server.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaex solution\n",
    "\n",
    "Vaex solutions are much more **powerful** and allow for easier feature engineering and scale.    \n",
    "In this example we do a simple feature engineering, and process the results to labels, so it would be easier to consume on the frontend side.\n",
    "\n",
    "* We do not need to implement transformers for each feature engineering step or estimators. Instead we create simple functions which does what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:54:39.349564Z",
     "start_time": "2021-11-16T17:54:33.655135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline raw data example:\n",
      "{\n",
      "    \"sepal_length\": 5.9,\n",
      "    \"sepal_width\": 3.0,\n",
      "    \"petal_length\": 4.2,\n",
      "    \"petal_width\": 1.5\n",
      "}\n",
      "\n",
      "Pipeline output example:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  sepal_length</th><th style=\"text-align: right;\">  sepal_width</th><th style=\"text-align: right;\">  petal_length</th><th style=\"text-align: right;\">  petal_width</th><th style=\"text-align: right;\">  class_</th><th style=\"text-align: right;\">  petal_ratio</th><th style=\"text-align: right;\">  prediction</th><th>label     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">           5.9</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">           4.2</td><td style=\"text-align: right;\">          1.5</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">      2.8    </td><td style=\"text-align: right;\">           1</td><td>versicolor</td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>1</i></td><td style=\"text-align: right;\">           6.1</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">           4.6</td><td style=\"text-align: right;\">          1.4</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">      3.28571</td><td style=\"text-align: right;\">           1</td><td>versicolor</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    sepal_length    sepal_width    petal_length    petal_width    class_    petal_ratio    prediction  label\n",
       "  0             5.9              3             4.2            1.5         1        2.8                 1  versicolor\n",
       "  1             6.1              3             4.6            1.4         1        3.28571             1  versicolor"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import vaex\n",
    "import warnings\n",
    "from vaex.ml.datasets import load_iris_1e5\n",
    "from vaex.ml.xgboost import XGBoostModel\n",
    "from goldilox import Pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df = load_iris_1e5()\n",
    "target = \"class_\"\n",
    "\n",
    "# feature engineering example\n",
    "df[\"petal_ratio\"] = df[\"petal_length\"] / df[\"petal_width\"]\n",
    "\n",
    "booster = XGBoostModel(\n",
    "    features=[\n",
    "        \"petal_length\",\n",
    "        \"petal_width\",\n",
    "        \"sepal_length\",\n",
    "        \"sepal_width\",\n",
    "        \"petal_ratio\",\n",
    "    ],\n",
    "    target=target,\n",
    "    prediction_name=\"prediction\",\n",
    "    num_boost_round=500,\n",
    ")\n",
    "booster.fit(df)\n",
    "df = booster.transform(df)\n",
    "\n",
    "df[\"label\"] = np.round(df[\"prediction\"]).map({0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"})\n",
    "\n",
    "# Vaex remember all the transformations, this is a skleran.pipeline alternative\n",
    "pipeline = Pipeline.from_vaex(df, description=\"simple lightGBM\")\n",
    "pipeline.raw.pop(target)  # (optional) we don't expect to get the class_ in queries\n",
    "assert pipeline.validate()\n",
    "print(\"Pipeline raw data example:\")\n",
    "print(json.dumps(pipeline.raw, indent=4))\n",
    "print(\"\")\n",
    "print(\"Pipeline output example:\")\n",
    "pipeline.inference(pipeline.raw).to_records()\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T17:54:55.395720Z",
     "start_time": "2021-11-16T17:54:41.417069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../tests/models/server.pkl\n",
      "Check out the docs: http://127.0.0.1:5000\n",
      "\n",
      "[2021-11-16 18:54:44 +0100] [74906] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-11-16 18:54:44 +0100] [74906] [INFO] Listening at: http://127.0.0.1:5000 (74906)\n",
      "[2021-11-16 18:54:44 +0100] [74906] [INFO] Using worker: uvicorn.workers.UvicornH11Worker\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Booting worker with pid: 74911\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Started server process [74911]\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Waiting for application startup.\n",
      "[2021-11-16 18:54:44 +0100] [74911] [INFO] Application startup complete.\n",
      "^C\n",
      "[2021-11-16 18:54:54 +0100] [74906] [INFO] Handling signal: int\n",
      "[2021-11-16 18:54:54 +0100] [74906] [WARNING] Worker with pid 74911 was terminated due to signal 3\n",
      "[2021-11-16 18:54:55 +0100] [74906] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved to: {pipeline.save('../tests/models/server.pkl')}\")\n",
    "print(f\"Check out the docs: http://127.0.0.1:5000\\n\")\n",
    "\n",
    "!gl serve ../tests/models/server.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance   \n",
    "Let's have a look at an edance training function, which we want to re-run when new data arrives.     \n",
    "To implement this, we must everything within a function which recive a dataframe and return a Vaex DataFrame\n",
    "\n",
    "The function:    \n",
    "First we run a \"*random_split*\" experiment and save the results.    \n",
    "Next, we train the data on the entire dataset.    \n",
    "Finally, we add the evalution as a varaible so we can recall how good the model was.\n",
    "\n",
    "\n",
    "* This way we can change the pipeline training and outputs without changing our infrastructure at all.\n",
    "* This also create a model for production who learned from the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:00:42.722945Z",
     "start_time": "2021-11-16T18:00:41.975205Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from vaex.ml.datasets import load_iris\n",
    "from goldilox import Pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # lightgbm fun\n",
    "\n",
    "\n",
    "def fit(df):\n",
    "    import vaex\n",
    "    import numpy as np\n",
    "    from vaex.ml.lightgbm import LightGBMModel\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from goldilox import Pipeline\n",
    "\n",
    "    train, test = df.ml.train_test_split(test_size=0.2, verbose=False)\n",
    "\n",
    "    features = [\"petal_length\", \"petal_width\", \"sepal_length\", \"sepal_width\"]\n",
    "    target = \"class_\"\n",
    "\n",
    "    booster = LightGBMModel(\n",
    "        features=features,\n",
    "        target=target,\n",
    "        prediction_name=\"predictions\",\n",
    "        num_boost_round=500,\n",
    "        params={\"verbose\": -1, \"objective\": \"multiclass\", \"num_class\": 3},\n",
    "    )\n",
    "    booster.fit(df)\n",
    "\n",
    "    @vaex.register_function()\n",
    "    def argmax(ar, axis=1):\n",
    "        return np.argmax(ar, axis=axis)\n",
    "\n",
    "    train = booster.transform(df)\n",
    "    train.add_function(\"argmax\", argmax)\n",
    "    train[\"prediction\"] = train[\"predictions\"].argmax()\n",
    "\n",
    "    \"\"\"\n",
    "    Using the  way to get predictions on a new dataset.\n",
    "    This is very helpful if we did many feature engineering transformations. \n",
    "    \"\"\"\n",
    "    pipeline = Pipeline.from_vaex(train)\n",
    "    accuracy = accuracy_score(\n",
    "        pipeline.inference(test[features])[\"prediction\"].values, test[target].values\n",
    "    )\n",
    "\n",
    "    # Re-train on the entire dataset\n",
    "    booster = LightGBMModel(\n",
    "        features=features,\n",
    "        target=target,\n",
    "        prediction_name=\"predictions\",\n",
    "        num_boost_round=500,\n",
    "        params={\"verbose\": -1, \"objective\": \"multiclass\", \"num_class\": 3},\n",
    "    )\n",
    "    booster.fit(df)\n",
    "    df = booster.transform(df)\n",
    "    df.add_function(\"argmax\", argmax)\n",
    "    df[\"prediction\"] = df[\"predictions\"].argmax()\n",
    "    # The 'label' is to help the Frontend app to understand what actually was the result\n",
    "    df[\"label\"] = df[\"prediction\"].map({0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"})\n",
    "    df.variables[\"accuracy\"] = accuracy\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_iris()\n",
    "pipeline = Pipeline.from_vaex(df, fit=fit).fit(df)\n",
    "pipeline.validate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:00:43.880434Z",
     "start_time": "2021-11-16T18:00:43.827868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  sepal_length</th><th style=\"text-align: right;\">  sepal_width</th><th style=\"text-align: right;\">  petal_length</th><th style=\"text-align: right;\">  petal_width</th><th style=\"text-align: right;\">  class_</th><th>predictions                                        </th><th style=\"text-align: right;\">  prediction</th><th>label     </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">           5.9</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">           4.2</td><td style=\"text-align: right;\">          1.5</td><td style=\"text-align: right;\">       1</td><td>&#x27;array([5.44126596e-08, 9.99999944e-01, 1.098061...</td><td style=\"text-align: right;\">           1</td><td>versicolor</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    sepal_length    sepal_width    petal_length    petal_width    class_  predictions                                            prediction  label\n",
       "  0             5.9              3             4.2            1.5         1  'array([5.44126596e-08, 9.99999944e-01, 1.098061...             1  versicolor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "path = str(TemporaryDirectory().name) + \"/model.pkl\"\n",
    "pipeline.save(path)\n",
    "pipeline = Pipeline.from_file(path)\n",
    "\n",
    "pipeline.inference(pipeline.raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:00:51.709816Z",
     "start_time": "2021-11-16T18:00:46.584400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaex.ml.datasets import load_iris_1e5\n",
    "\n",
    "df = load_iris_1e5()\n",
    "pipeline.fit(df)\n",
    "pipeline.validate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve\n",
    "\n",
    "* Note that when we train in this way, the \"*raw*\" example has the target variable \"class_\" which we will not expect in production.  This is no issue, we can either \"pop\" it out from the pipeline.raw, or just ignore it, predictions still work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:01:46.165013Z",
     "start_time": "2021-11-16T18:01:22.732040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../tests/models/server.pkl\n",
      "Check out the docs: http://127.0.0.1:5000\n",
      "\n",
      "[2021-11-16 19:01:25 +0100] [75207] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-11-16 19:01:25 +0100] [75207] [INFO] Listening at: http://127.0.0.1:5000 (75207)\n",
      "[2021-11-16 19:01:25 +0100] [75207] [INFO] Using worker: uvicorn.workers.UvicornH11Worker\n",
      "[2021-11-16 19:01:25 +0100] [75213] [INFO] Booting worker with pid: 75213\n",
      "[2021-11-16 19:01:26 +0100] [75213] [INFO] Started server process [75213]\n",
      "[2021-11-16 19:01:26 +0100] [75213] [INFO] Waiting for application startup.\n",
      "[2021-11-16 19:01:26 +0100] [75213] [INFO] Application startup complete.\n",
      "^C\n",
      "[2021-11-16 19:01:45 +0100] [75207] [INFO] Handling signal: int\n",
      "[2021-11-16 19:01:45 +0100] [75207] [WARNING] Worker with pid 75213 was terminated due to signal 3\n",
      "[2021-11-16 19:01:45 +0100] [75207] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "pipeline.raw.pop('class_', None)\n",
    "print(f\"Saved to: {pipeline.save('../tests/models/server.pkl')}\")\n",
    "print(f\"Check out the docs: http://127.0.0.1:5000\\n\")\n",
    "\n",
    "!gl serve ../tests/models/server.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}