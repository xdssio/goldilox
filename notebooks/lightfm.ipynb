{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LightFM](https://github.com/lyst/lightfm)\n",
    "[Docs](http://lyst.github.io/lightfm/docs/home.html)\n",
    "\n",
    "Recommednation engines are complciated. \n",
    "There is much do discuss, but I will be brief.\n",
    "Check out [this exmaple] (https://making.lyst.com/lightfm/docs/examples/hybrid_crossvalidated.html) and [this](https://medium.com/analytics-vidhya/matrix-factorization-made-easy-recommender-systems-7e4f50504477)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In real life, data doesn't arrive in nice sparse matrices - \n",
    "so we turn it to tabular as a reasonable starting point"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T08:24:10.029719Z",
     "start_time": "2022-01-16T08:24:00.791610Z"
    },
    "hidden": true
   },
   "source": [
    "import numpy as np\n",
    "import vaex\n",
    "import pyarrow as pa\n",
    "from lightfm.datasets import fetch_stackexchange\n",
    "\n",
    "data = fetch_stackexchange('crossvalidated',\n",
    "                           test_set_fraction=0.1,\n",
    "                           indicator_features=False,\n",
    "                           tag_features=True)\n",
    "train, test = data['train'], data['test']\n",
    "\n",
    "item_features = data['item_features']\n",
    "print(f\"The dataset has {train.shape[0]} users and {train.shape[1]} items,\\\n",
    "\\nwith {test.getnnz()} interactions in the test and {train.getnnz()} \\\n",
    "\\ninteractions in the training set.\")\n",
    "\n",
    "tag_labels = {i:tag for i,tag in enumerate(data['item_feature_labels'])}\n",
    "tags = {}\n",
    "for i in range(item_features.shape[0]):\n",
    "    tags[i] = [tag_labels.get(i) for i in item_features[i].tocoo().col]\n",
    "\n",
    "def to_vaex(s, label):\n",
    "    dok = s.todok()\n",
    "    users = []\n",
    "    question = []\n",
    "    stars = []\n",
    "    item_tags = []\n",
    "    for key, value in dok.items():\n",
    "        users.append(key[0])\n",
    "        question.append(key[1])\n",
    "        item_tags.append(tags.get(key[1]))\n",
    "    dataset = [label]*len(users)\n",
    "    \n",
    "    df = vaex.from_arrays(user=users, \n",
    "                          question=question, \n",
    "                          dataset=dataset,\n",
    "                         tags=pa.array(item_tags))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = to_vaex(train, 'train').concat(to_vaex(test,'test'))\n",
    "df.export_parquet('data/stack_exchange.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "An optional helper vaex class for getting sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:03:27.042214Z",
     "start_time": "2022-01-16T09:03:26.987908Z"
    },
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  user</th><th style=\"text-align: right;\">  question</th><th>dataset  </th><th>tags                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         0</td><td>train    </td><td>[&#x27;bayesian&#x27;, &#x27;prior&#x27;, &#x27;elicitation&#x27;]</td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>1</i></td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         0</td><td>train    </td><td>[&#x27;bayesian&#x27;, &#x27;prior&#x27;, &#x27;elicitation&#x27;]</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    user    question  dataset    tags\n",
       "  0       3           0  train      ['bayesian', 'prior', 'elicitation']\n",
       "  1      13           0  train      ['bayesian', 'prior', 'elicitation']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vaex\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix,dok_matrix\n",
    "import pyarrow as pa\n",
    "\n",
    "@vaex.register_dataframe_accessor('sparse', override=True)\n",
    "class Sparse(object):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def to_csr(self):\n",
    "        # in case we get new qustions \n",
    "        max_question = self.df.variables['max_question']\n",
    "        max_user = self.df.variables['max_user']\n",
    "        data = self.df[(self.df['question'] < max_question) &\n",
    "               (self.df['user'] < max_user)]\n",
    "        length = len(data)\n",
    "        return csr_matrix((np.ones(length), \n",
    "                   (data['user'].values, data['question'].values)), \n",
    "                  shape=(max_user, max_question))\n",
    "    \n",
    "    def side_features(self):\n",
    "        tag_count = self.df.variables['tags_count']\n",
    "        max_question = self.df.variables['max_question']\n",
    "        data = self.df[(self.df['question'] < max_question)]\n",
    "        S = dok_matrix((max_question + 1, \n",
    "                        tag_count + 1), \n",
    "                        dtype=np.int32)\n",
    "        cache = set()\n",
    "        qustions = data.question.tolist()\n",
    "        tags_ids = data.tags_ids.tolist()\n",
    "        for row, row_tags in zip(qustions, tags_ids):\n",
    "            if row not in cache:\n",
    "                for t in row_tags:\n",
    "                    if t is not None:\n",
    "                        S[row, t] = 1\n",
    "                cache.add(row)\n",
    "        return S.tocsr()\n",
    "    \n",
    "    def get_similar_tags(model, tag_id):\n",
    "        # Define similarity as the cosine of the angle\n",
    "        # between the tag latent vectors\n",
    "\n",
    "        # Normalize the vectors to unit length\n",
    "        tag_embeddings = (model.item_embeddings.T\n",
    "                          / np.linalg.norm(model.item_embeddings, axis=1)).T\n",
    "\n",
    "        query_embedding = tag_embeddings[tag_id]\n",
    "        similarity = np.dot(tag_embeddings, query_embedding)\n",
    "        most_similar = np.argsort(-similarity)[1:4]\n",
    "\n",
    "        return most_similar\n",
    "\n",
    "\n",
    "df = vaex.open('data/stack_exchange.parquet')\n",
    "train = df[df['dataset']=='train']\n",
    "test = df[df['dataset']=='test']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reality vs Toy examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have all the possible tags ahead of time, you can work with all possible tags, and questions. It means to count the tags with \"df\" and not with \"train\".\n",
    "\n",
    "In reality you might, you might get new tags in production...\n",
    "\n",
    "A similar issue is with the shape of the sparse matrix. \n",
    "If you only have the train data at the begining - the \"max_question\" and \"max_user\" should be the max from the train instead of the df."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Best case - \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Use df\n",
    "tags = {tag:i for i,tag in enumerate( \n",
    "        set([item for sublist in df['tags'].tolist() for item in sublist]))}\n",
    "\n",
    "@vaex.register_function(on_expression=True)\n",
    "def ids(ar):    \n",
    "    return np.array([[tags.get(tag) for tag in x] for x in ar.tolist()])\n",
    "\n",
    "train.add_function('ids', ids)\n",
    "train['tags_ids'] = train.func.ids('tags')\n",
    "train.variables['tags_count'] = len(tags)\n",
    "train.variables['max_question'] = int(df.question.max()) #  using df\n",
    "train.variables['max_user'] = int(df.user.max()) #  using df\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To challange ourselves, we will pretend we don't have the test data at all, which is worse case.   \n",
    "This get's us a more rubust modeling and realistic results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:03:39.758792Z",
     "start_time": "2022-01-16T09:03:39.187988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  user</th><th style=\"text-align: right;\">  question</th><th>dataset  </th><th>tags                                </th><th>tags_ids                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         0</td><td>train    </td><td>[&#x27;bayesian&#x27;, &#x27;prior&#x27;, &#x27;elicitation&#x27;]</td><td>array([1038, 967, 931], dtype=object)</td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>1</i></td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         0</td><td>train    </td><td>[&#x27;bayesian&#x27;, &#x27;prior&#x27;, &#x27;elicitation&#x27;]</td><td>array([1038, 967, 931], dtype=object)</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    user    question  dataset    tags                                  tags_ids\n",
       "  0       3           0  train      ['bayesian', 'prior', 'elicitation']  array([1038, 967, 931], dtype=object)\n",
       "  1      13           0  train      ['bayesian', 'prior', 'elicitation']  array([1038, 967, 931], dtype=object)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tags = {tag:i for i,tag in enumerate(\n",
    "    set([item for sublist in train['tags'].tolist() for item in sublist]))}\n",
    "\n",
    "@vaex.register_function(on_expression=True)\n",
    "def ids(ar):    \n",
    "    return np.array([[tags.get(tag) for tag in x] for x in ar.tolist()], dtype=object)\n",
    "\n",
    "train.add_function('ids', ids)\n",
    "train['tags_ids'] = train.func.ids('tags')\n",
    "train.variables['tags_count'] = len(tags)\n",
    "train.variables['max_question'] = int(train.question.max())\n",
    "train.variables['max_user'] = int(train.user.max())\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T18:17:46.910633Z",
     "start_time": "2022-01-15T18:17:46.894054Z"
    }
   },
   "source": [
    "# A pure collaborative filtering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:03:53.230067Z",
     "start_time": "2022-01-16T09:03:40.255663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 219 ms, sys: 13.5 ms, total: 232 ms\n",
      "Wall time: 233 ms\n",
      "Collaborative filtering train AUC: 0.8706144\n"
     ]
    }
   ],
   "source": [
    "# Import the model\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score\n",
    "# Set the number of threads; you can increase this\n",
    "# if you have more physical cores available.\n",
    "NUM_THREADS = 16\n",
    "NUM_COMPONENTS = 30\n",
    "NUM_EPOCHS = 3\n",
    "ITEM_ALPHA = 1e-6\n",
    "\n",
    "# Let's fit a WARP model: these generally have the best performance.\n",
    "model = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "               no_components=NUM_COMPONENTS)\n",
    "\n",
    "# Run 3 epochs and time it.\n",
    "train_csr = train.sparse.to_csr()\n",
    "%time model = model.fit(train_csr, epochs=NUM_EPOCHS, num_threads=NUM_THREADS)\n",
    "\n",
    "train_auc = auc_score(model, train_csr, num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering train AUC: %s' % train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:03:58.010579Z",
     "start_time": "2022-01-16T09:03:55.097682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:WARNING: Pipeline doesn't handle NA for tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering test AUC: 0.53614616\n"
     ]
    }
   ],
   "source": [
    "from goldilox import Pipeline\n",
    "\n",
    "pipeline = Pipeline.from_vaex(train)\n",
    "test = pipeline.inference(test)\n",
    "test_csr = test.sparse.to_csr()\n",
    "\n",
    "test_auc = auc_score(model, test_csr, train_interactions=train_csr, num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering test AUC: %s' % test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very well - a cold start problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use side features\n",
    "The StackExchange data comes with content information in the form of tags users apply to their questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:04:35.401797Z",
     "start_time": "2022-01-16T09:04:18.786660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid training set AUC: 0.8841671347618103\n"
     ]
    }
   ],
   "source": [
    "# Define a new model instance\n",
    "model = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                no_components=NUM_COMPONENTS)\n",
    "\n",
    "\n",
    "train_item_features = train.sparse.side_features()\n",
    "model = model.fit(train_csr,\n",
    "                item_features=train_item_features,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                num_threads=NUM_THREADS)\n",
    "\n",
    "train_auc = auc_score(model,\n",
    "                      train_csr,\n",
    "                      item_features=train_item_features,\n",
    "                      num_threads=NUM_THREADS).mean()\n",
    "print(f\"Hybrid training set AUC: {train_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we do now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:05:13.413890Z",
     "start_time": "2022-01-16T09:05:11.618386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid test set AUC: 0.91697025\n"
     ]
    }
   ],
   "source": [
    "test_item_features = test.sparse.side_features()\n",
    "test_auc = auc_score(model,\n",
    "                    test_csr,\n",
    "                    train_interactions=train_csr,\n",
    "                    item_features=test_item_features,\n",
    "                    num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test set AUC: %s' % test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get it into prodction\n",
    "* We use all the data.\n",
    "* We create a recommendation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:05:30.041268Z",
     "start_time": "2022-01-16T09:05:29.453040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                            </th><th style=\"text-align: right;\">  user</th><th style=\"text-align: right;\">  question</th><th>dataset  </th><th>tags                                </th><th>tags_ids                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td><i style='opacity: 0.6'>0</i></td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         0</td><td>train    </td><td>[&#x27;bayesian&#x27;, &#x27;prior&#x27;, &#x27;elicitation&#x27;]</td><td>array([1038, 967, 931], dtype=object)</td></tr>\n",
       "<tr><td><i style='opacity: 0.6'>1</i></td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         0</td><td>train    </td><td>[&#x27;bayesian&#x27;, &#x27;prior&#x27;, &#x27;elicitation&#x27;]</td><td>array([1038, 967, 931], dtype=object)</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "  #    user    question  dataset    tags                                  tags_ids\n",
       "  0       3           0  train      ['bayesian', 'prior', 'elicitation']  array([1038, 967, 931], dtype=object)\n",
       "  1      13           0  train      ['bayesian', 'prior', 'elicitation']  array([1038, 967, 931], dtype=object)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {tag:i for i,tag in enumerate(set([item for sublist in train['tags'].tolist() for item in sublist]))}\n",
    "\n",
    "@vaex.register_function(on_expression=True)\n",
    "def ids(ar):    \n",
    "    return np.array([[tags.get(tag) for tag in x] for x in ar.tolist()], dtype=object)\n",
    "\n",
    "df.add_function('ids', ids)\n",
    "df['tags_ids'] = train.func.ids('tags')\n",
    "df.variables['tags_count'] = len(tags)\n",
    "df.variables['max_question'] = int(df.question.max())\n",
    "df.variables['max_user'] = int(df.user.max())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:06:23.864371Z",
     "start_time": "2022-01-16T09:06:21.463633Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                no_components=NUM_COMPONENTS)\n",
    "\n",
    "\n",
    "item_features = df.sparse.side_features()\n",
    "model = model.fit(df.sparse.to_csr(),\n",
    "                item_features=item_features,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                num_threads=NUM_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:10:27.826216Z",
     "start_time": "2022-01-16T09:10:21.943432Z"
    }
   },
   "outputs": [],
   "source": [
    "# groupby-concatenate currently not supported in vaex\n",
    "users = df[['user','question']].to_pandas_df() \n",
    "users_history = users.groupby(['user'])['question'].apply(list).to_dict()\n",
    "qustions = set(df['question'].unique())\n",
    "users_options = {user: qustions.difference(history) for user, history in users_history.items()}\n",
    "most_popular = list(df['question'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:16:36.958003Z",
     "start_time": "2022-01-16T09:16:36.945214Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@vaex.register_function()\n",
    "def recommend(ar, topk=5):\n",
    "    ret = []\n",
    "    for user in ar.tolist():\n",
    "        user_options = list(users_options.get(user))\n",
    "        if not user_options:\n",
    "            ret.append(most_popular)\n",
    "        else:\n",
    "            # cool way to sort topk\n",
    "            recommendations = model.predict(np.repeat(user, len(user_options)), \n",
    "                                            user_options,\n",
    "                                            item_features=item_features).argsort()[-topk:][\n",
    "                              ::-1]\n",
    "            if len(recommendations) == 0:\n",
    "                recommendations = most_popular\n",
    "            ret.append(recommendations)\n",
    "    return np.array(ret)\n",
    "df.add_function('recommend', recommend)\n",
    "df['recommendations'] = df.user.recommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-16T09:21:20.795Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_vaex(df)\n",
    "pipeline.raw = {\"user\":5}\n",
    "pipeline.inference(pipeline.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might looks like a bit of a complicated way to do it.   \n",
    "Recommendation engines are not very simple, and there are many edge cases.   \n",
    "Thing to look out for - which should not crash your solution:\n",
    "* New users.\n",
    "* New tags.\n",
    "* No user.\n",
    "* No tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c02e8cefd04ff52e799f4aa259d2ee492875245d06169a1d386f6f6b41a66828"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
